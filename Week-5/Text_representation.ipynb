{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. BOW with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        and  document     first        is       one    second       the  \\\n",
      "0  0.000000  0.469791  0.580286  0.384085  0.000000  0.000000  0.384085   \n",
      "1  0.000000  0.687624  0.000000  0.281089  0.000000  0.538648  0.281089   \n",
      "2  0.511849  0.000000  0.000000  0.267104  0.511849  0.000000  0.267104   \n",
      "3  0.000000  0.469791  0.580286  0.384085  0.000000  0.000000  0.384085   \n",
      "\n",
      "      third      this  \n",
      "0  0.000000  0.384085  \n",
      "1  0.000000  0.281089  \n",
      "2  0.511849  0.267104  \n",
      "3  0.000000  0.384085  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Sample corpus\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?'\n",
    "]\n",
    "\n",
    "# Create the Document-Term Matrix using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Convert to DataFrame for better visualization\n",
    "df_tfidf = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Display the TF-IDF representation\n",
    "print(df_tfidf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. N-Gram Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({('This', 'is'): 1, ('is', 'a'): 1, ('a', 'sample'): 1, ('sample', 'text'): 1, ('text', 'for'): 1, ('for', 'n-gram'): 1, ('n-gram', 'generation.'): 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Sample text\n",
    "text = \"This is a sample text for n-gram generation.\"\n",
    "\n",
    "# Tokenize the text\n",
    "words = text.split()\n",
    "\n",
    "# Generate n-grams (bigrams in this case)\n",
    "bigrams = [(words[i], words[i + 1]) for i in range(len(words) - 1)]\n",
    "\n",
    "# Count the frequency of each bigram\n",
    "bigram_freq = Counter(bigrams)\n",
    "\n",
    "# Display the bigram frequency\n",
    "print(bigram_freq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c. Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for the word \"this\": [-0.07511582 -0.00930042  0.09538119 -0.07319167 -0.02333769 -0.01937741\n",
      "  0.08077437 -0.05930896  0.00045162 -0.04753734]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Sample sentences for Word2Vec training\n",
    "sentences = [\n",
    "    'This is the first sentence.',\n",
    "    'This is the second sentence.',\n",
    "    'And here is the third one.',\n",
    "    'Finally, this is the fourth sentence.'\n",
    "]\n",
    "\n",
    "# Tokenize the sentences\n",
    "tokenized_sentences = [sentence.lower().split() for sentence in sentences]\n",
    "\n",
    "# Create the Word2Vec model\n",
    "model = Word2Vec(sentences=tokenized_sentences, vector_size=10, window=2, min_count=1, sg=0)\n",
    "\n",
    "# Get the vector for a specific word\n",
    "vector = model.wv['this']\n",
    "\n",
    "# Display the vector\n",
    "print('Vector for the word \"this\":', vector)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d. Glove "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulated GloVe vector for the word \"example\": [0.27274251 0.41963384 0.79067113 0.45911916 0.00110565 0.08710738\n",
      " 0.55445311 0.05676897 0.70328141 0.5923694  0.4698087  0.61315844\n",
      " 0.71531048 0.27622761 0.35904167 0.94381736 0.34031891 0.91601064\n",
      " 0.29009743 0.7937432  0.01070652 0.876937   0.70900458 0.5136154\n",
      " 0.32041572 0.2062632  0.995412   0.99250027 0.43574548 0.42767655\n",
      " 0.37661958 0.27493133 0.98377642 0.94132723 0.20515455 0.97571924\n",
      " 0.8033774  0.75329976 0.74977859 0.5038612  0.69606329 0.34119061\n",
      " 0.9482752  0.91473415 0.51143974 0.9596004  0.78856035 0.0723658\n",
      " 0.51319278 0.15240522 0.53057653 0.3974397  0.88842965 0.46733462\n",
      " 0.55281549 0.01891567 0.52942749 0.6269131  0.22858334 0.9320803\n",
      " 0.16326369 0.67968603 0.14675929 0.85051569 0.91400214 0.50856876\n",
      " 0.4569321  0.88352817 0.83411403 0.74013396 0.11313267 0.63608553\n",
      " 0.25472575 0.43870608 0.2838239  0.0205455  0.42450024 0.91882632\n",
      " 0.65142149 0.74664274 0.59439701 0.35523863 0.54192955 0.17260612\n",
      " 0.43340557 0.90859172 0.15283565 0.8108426  0.55512088 0.80246784\n",
      " 0.6148225  0.08707326 0.56835852 0.02966333 0.65166988 0.25068046\n",
      " 0.27989283 0.90427363 0.33031664 0.81976969]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Simulating a vector for a word\n",
    "example_vector = np.random.rand(100) # Simulated vector\n",
    "\n",
    "# Display a simulated GloVe vector\n",
    "print('Simulated GloVe vector for the word \"example\":', example_vector)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e. Fast Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for the word \"this\": [-0.02639857 -0.01558776 -0.00070657 -0.00016564 -0.02406672  0.01902237\n",
      "  0.00485064  0.01075732 -0.00732209 -0.01498174]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "# Sample sentences for FastText training\n",
    "sentences = [\n",
    "    'This is the first sentence.',\n",
    "    'This is the second sentence.',\n",
    "    'And here is the third one.',\n",
    "    'Finally, this is the fourth sentence.'\n",
    "]\n",
    "\n",
    "# Tokenize the sentences\n",
    "tokenized_sentences = [sentence.lower().split() for sentence in sentences]\n",
    "\n",
    "# Create the FastText model\n",
    "model = FastText(sentences=tokenized_sentences, vector_size=10, window=2, min_count=1, sg=0)\n",
    "\n",
    "# Get the vector for a specific word\n",
    "vector = model.wv['this']\n",
    "\n",
    "# Display the vector\n",
    "print('Vector for the word \"this\":', vector)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## f. Setence Embedding Technique: Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for the document \"D1\": [-0.05208231 -0.05845232 -0.1017203   0.08560619  0.03617087  0.00102675\n",
      " -0.10051455 -0.05322951 -0.09929331  0.01917398]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "# Sample documents for Doc2Vec training\n",
    "documents = [\n",
    "    TaggedDocument(words='This is the first document.'.split(), tags=['D1']),\n",
    "    TaggedDocument(words='This document is the second document.'.split(), tags=['D2']),\n",
    "    TaggedDocument(words='And this is the third one.'.split(), tags=['D3']),\n",
    "    TaggedDocument(words='Is this the first document?'.split(), tags=['D4'])\n",
    "]\n",
    "\n",
    "# Create the Doc2Vec model\n",
    "model = Doc2Vec(documents, vector_size=10, window=2, min_count=1, epochs=40)\n",
    "\n",
    "# Get the vector for a specific document\n",
    "vector = model.dv['D1']\n",
    "\n",
    "# Display the vector\n",
    "print('Vector for the document \"D1\":', vector)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## g. Transformer - BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rikato/miniconda3/envs/NLP/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertTokenizer, BertModel\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load pre-trained model tokenizer\u001b[39;00m\n\u001b[1;32m      5\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m BertTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained model tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Encode text\n",
    "text = \"This is a sample text for BERT embeddings.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "# Load pre-trained model\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Get the embeddings\n",
    "with torch.no_grad():\n",
    "    output = model(**encoded_input)\n",
    "\n",
    "# Extract the embeddings for the [CLS] token\n",
    "cls_embedding = output.last_hidden_state[:, 0, :].squeeze()\n",
    "\n",
    "# Display the embedding\n",
    "print('BERT embedding for the sentence:', cls_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
