{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutorials:\n",
    "1. [ Natural Language Processing (NLP) Tutorial with Python & NLTK](https://youtu.be/X2vAabgKiuM)\n",
    "2. [Spacy Introduction for NLP](https://www.youtube.com/watch?v=pLJm0WSIVDk&list=PLc2rvfiptPSSS-iwKS_lxI3MZr8Mbi4Zu)\n",
    "3. [TextBlob Library In Python For Natural Language Processing](https://www.youtube.com/watch?v=DedS74YKhs4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ntlk.tokenize:\n",
    "    - Provides functions for splitting text into sentences and words\n",
    "    - sent_tokenize: splits text into sentences\n",
    "    - word_tokenize: splits text into words\n",
    "2. ntlk.corpus.stopwords:\n",
    "    - Provides a list of common words (the, and, is) that are often removed from text since it does not carry meaning\n",
    "3. ntlk.stem.PorterStemmer:\n",
    "    - Provides a simple algorithm for reducing words to their root form (or stem)\n",
    "    - Running => Runs and Ran => Run\n",
    "4. ntlk.stem.WordNetLemmatizer:\n",
    "    - Provides a more sophisticated algorithm for reducing words to their root form (or lemma). Unlike stemming, lemmatization takes into account the context of the word and returns the base form of the word that is a valid English word\n",
    "    - Better => Good\n",
    "5. textblob:\n",
    "    - Provides a simple API for common NLP tasks, such as part-of-speech tagging, noun phrase extraction, sentiment analysis, and language transition\n",
    "6. enchant:\n",
    "    - Provides a simple API for spell checking text in various languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import string\n",
    "from collections import Counter\n",
    "from textblob import TextBlob\n",
    "import enchant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional resources for NLTK libarary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **punkt**: Pre-trained model that can be used to split text into sentences\n",
    "2. **stopwords**: Downloads a list of common English stopwords, which are words that are often removed since they don't carry much meaning\n",
    "3. **wordnet**: WordNet lexical database, which is a large collection of English words and their meaning. WordNet includes synsets (sets of synonyms), hypernyms (more general words), and hyponyms (more specific words), which can be used to understand the relationships between words and to perform tasks such as word sense disambiguation and semantic similarity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/rikato/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/rikato/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/rikato/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week - 1\n",
    "1. Lexical Analysis - Preprocessing Text\n",
    "\n",
    "   - Write a program to remove punctuations, special symbols, numbers using regular expression\n",
    "   - Tokenize the given text (Tokenizing the text into sentences)\n",
    "   - Add Custom Stopwords and List Removed Stopwords\n",
    "   - Perform stemming and lemmatization on text\n",
    "   - Extract the usernames from the email addresses present\n",
    "   - Find the most common words in the text to exclude as stopwords\n",
    "   - Write a program to Correct the spelling errors using textblob\n",
    "\n",
    "2. Apply Level on Text Preprocessing\n",
    "\n",
    "    -  Replace social media slangs from a text (create dictionary)\n",
    "    -  Apply stemming or lemmatization only if the word doesn't have any meaning in a dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lexical Analysis - Preprocessing Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sample Text for Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text for demonstration\n",
    "sample_text = \"\"\"\n",
    "Hello! This is a sample text for our NLP experiment. It contains punctuation, numbers (like 123), and special symbols (@#$).\n",
    "We'll perform various operations on this text. Some email addresses: john.doe@example.com, jane_smith@company.org.\n",
    "This text also includes some common words and potential spelling errors, such as 'langauge' instead of 'language'.\n",
    "NLP is gr8 for txt analysis. ROFL!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "\n",
      "Hello! This is a sample text for our NLP experiment. It contains punctuation, numbers (like 123), and special symbols (@#$).\n",
      "We'll perform various operations on this text. Some email addresses: john.doe@example.com, jane_smith@company.org.\n",
      "This text also includes some common words and potential spelling errors, such as 'langauge' instead of 'language'.\n",
      "NLP is gr8 for txt analysis. ROFL!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Text:\")\n",
    "print(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a. Remove punctuations, special symbols, numbers using regular expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1a. Remove punctuations, special symbols, numbers using regular expression\n",
    "def remove_punct_symbols_numbers(text):\n",
    "    return re.sub(r'[^\\w\\s]|\\d', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text after removing punctuations, special symbols, and numbers:\n",
      "\n",
      "Hello This is a sample text for our NLP experiment It contains punctuation numbers like  and special symbols \n",
      "Well perform various operations on this text Some email addresses johndoeexamplecom jane_smithcompanyorg\n",
      "This text also includes some common words and potential spelling errors such as langauge instead of language\n",
      "NLP is gr for txt analysis ROFL\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nText after removing punctuations, special symbols, and numbers:\")\n",
    "cleaned_text = remove_punct_symbols_numbers(sample_text)\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b. Tokenize the text into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1b. Tokenize the text into sentences\n",
    "def tokenize_sentences(text):\n",
    "    return sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenized sentences:\n",
      "Sentence 1: \n",
      "Hello!\n",
      "Sentence 2: This is a sample text for our NLP experiment.\n",
      "Sentence 3: It contains punctuation, numbers (like 123), and special symbols (@#$).\n",
      "Sentence 4: We'll perform various operations on this text.\n",
      "Sentence 5: Some email addresses: john.doe@example.com, jane_smith@company.org.\n",
      "Sentence 6: This text also includes some common words and potential spelling errors, such as 'langauge' instead of 'language'.\n",
      "Sentence 7: NLP is gr8 for txt analysis.\n",
      "Sentence 8: ROFL!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTokenized sentences:\")\n",
    "sentences = tokenize_sentences(sample_text)\n",
    "for i, sentence in enumerate(sentences, 1):\n",
    "    print(f\"Sentence {i}: {sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### c. Add Custom Stopwords and List Removed Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1c. Add Custom Stopwords and List Removed Stopwords\n",
    "def remove_stopwords(text, custom_stopwords=[]):\n",
    "    stop_words = set(stopwords.words('english') + custom_stopwords)\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    removed_stopwords = [word for word in words if word.lower() in stop_words]\n",
    "    return ' '.join(filtered_words), removed_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text after removing stopwords (including custom stopwords):\n",
      "Hello text NLP experiment punctuation numbers like special symbols Well perform various operations text email addresses johndoeexamplecom jane_smithcompanyorg text also includes common words potential spelling errors langauge instead language NLP gr txt analysis ROFL\n",
      "Removed stopwords: ['This', 'is', 'a', 'sample', 'for', 'our', 'It', 'contains', 'and', 'on', 'this', 'Some', 'This', 'some', 'and', 'such', 'as', 'of', 'is', 'for']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nText after removing stopwords (including custom stopwords):\")\n",
    "custom_stopwords = ['sample', 'contains']\n",
    "text_without_stopwords, removed_stopwords = remove_stopwords(cleaned_text, custom_stopwords)\n",
    "print(text_without_stopwords)\n",
    "print(\"Removed stopwords:\", removed_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### d. Perform stemming and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1d. Perform stemming and lemmatization\n",
    "def stem_and_lemmatize(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = word_tokenize(text)\n",
    "    stemmed = [stemmer.stem(word) for word in words]\n",
    "    lemmatized = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(stemmed), ' '.join(lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stemmed and Lemmatized text:\n",
      "Stemmed: hello thi is a sampl text for our nlp experi it contain punctuat number like and special symbol well perform variou oper on thi text some email address johndoeexamplecom jane_smithcompanyorg thi text also includ some common word and potenti spell error such as langaug instead of languag nlp is gr for txt analysi rofl\n",
      "Lemmatized: Hello This is a sample text for our NLP experiment It contains punctuation number like and special symbol Well perform various operation on this text Some email address johndoeexamplecom jane_smithcompanyorg This text also includes some common word and potential spelling error such a langauge instead of language NLP is gr for txt analysis ROFL\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nStemmed and Lemmatized text:\")\n",
    "stemmed, lemmatized = stem_and_lemmatize(cleaned_text)\n",
    "print(\"Stemmed:\", stemmed)\n",
    "print(\"Lemmatized:\", lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### e. Extract usernames from email addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1e. Extract usernames from email addresses\n",
    "def extract_usernames(text):\n",
    "    email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
    "    emails = re.findall(email_pattern, text)\n",
    "    usernames = [email.split('@')[0] for email in emails]\n",
    "    return usernames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted usernames from email addresses:\n",
      "['john.doe', 'jane_smith']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nExtracted usernames from email addresses:\")\n",
    "usernames = extract_usernames(sample_text)\n",
    "print(usernames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### f. Find the most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1f. Find the most common words\n",
    "def find_common_words(text, n=10):\n",
    "    words = word_tokenize(text.lower())\n",
    "    word_freq = Counter(words)\n",
    "    return word_freq.most_common(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Most common words:\n",
      "this: 3\n",
      "text: 3\n",
      "is: 2\n",
      "for: 2\n",
      "nlp: 2\n",
      "and: 2\n",
      "some: 2\n",
      "hello: 1\n",
      "a: 1\n",
      "sample: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nMost common words:\")\n",
    "common_words = find_common_words(cleaned_text)\n",
    "for word, count in common_words:\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### g. Correct spelling errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1g. Correct spelling errors\n",
    "def correct_spelling(text):\n",
    "    return str(TextBlob(text).correct())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text after spelling correction:\n",
      "\n",
      "Hello! His is a sample text for our NLP experiment. It contains punctuation, numbers (like 123), and special symbols (@#$).\n",
      "He'll perform various operations on this text. Some email addresses: john.doe@example.com, jane_smith@company.org.\n",
      "His text also includes some common words and potential spelling errors, such as 'language' instead of 'language'.\n",
      "NLP is grm for txt analysis. ROFL!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nText after spelling correction:\")\n",
    "corrected_text = correct_spelling(sample_text)\n",
    "print(corrected_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply Level on Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a. Replace social media slangs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2a. Replace social media slangs\n",
    "def replace_slangs(text, slang_dict):\n",
    "    words = word_tokenize(text)\n",
    "    replaced_words = [slang_dict.get(word.lower(), word) for word in words]\n",
    "    return ' '.join(replaced_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text after replacing social media slangs:\n",
      "Hello ! This is a sample text for our NLP experiment . It contains punctuation , numbers ( like 123 ) , and special symbols ( @ # $ ) . We 'll perform various operations on this text . Some email addresses : john.doe @ example.com , jane_smith @ company.org . This text also includes some common words and potential spelling errors , such as 'langauge ' instead of 'language ' . NLP is great for text analysis . rolling on the floor laughing !\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nText after replacing social media slangs:\")\n",
    "slang_dict = {'gr8': 'great', 'txt': 'text', 'rofl': 'rolling on the floor laughing'}\n",
    "text_without_slangs = replace_slangs(sample_text, slang_dict)\n",
    "print(text_without_slangs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b. Apply stemming or lemmatization only if the word doesn't have meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2b. Apply stemming or lemmatization only if the word doesn't have meaning\n",
    "def smart_stem_lemmatize(text):\n",
    "    d = enchant.Dict(\"en_US\")\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = word_tokenize(text)\n",
    "    processed_words = []\n",
    "    for word in words:\n",
    "        if not d.check(word):\n",
    "            lemma = lemmatizer.lemmatize(word)\n",
    "            if d.check(lemma):\n",
    "                processed_words.append(lemma)\n",
    "            else:\n",
    "                processed_words.append(word)\n",
    "        else:\n",
    "            processed_words.append(word)\n",
    "    return ' '.join(processed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2b. Text after smart stemming/lemmatization:\n",
      "Hello This is a sample text for our NLP experiment It contains punctuation numbers like and special symbols Well perform various operations on this text Some email addresses johndoeexamplecom jane_smithcompanyorg This text also includes some common words and potential spelling errors such as langauge instead of language NLP is gr for txt analysis ROFL\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n2b. Text after smart stemming/lemmatization:\")\n",
    "smart_processed_text = smart_stem_lemmatize(cleaned_text)\n",
    "print(smart_processed_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".NLPvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
